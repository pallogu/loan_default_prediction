{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../etl/train_dataset_after_pca.csv\")\n",
    "eval_df = pd.read_csv(\"../etl/val_dataset_after_pca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.gym_market_env import CustomEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.2\n",
    "reward_multiplicator = 144.5\n",
    "negative_reward_multiplicator = 140.6\n",
    "\n",
    "train_py_env = CustomEnv(\n",
    "    trades = train,\n",
    "    features = [c for c in train.columns.values if \"f_\" in c] + [\"feature_0\", \"weight\"],\n",
    "    reward_column = \"resp\",\n",
    "    weight_column = \"weight\",\n",
    "    include_weight=True,\n",
    "    reward_multiplicator = reward_multiplicator,\n",
    "    negative_reward_multiplicator = negative_reward_multiplicator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(train_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DummyVecEnv([lambda: train_py_env])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=20000)\n",
    "obs = env.reset()\n",
    "for i in range(2000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
