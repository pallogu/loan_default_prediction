{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO with MLP policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from envs.gym_market_env import CustomEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../etl/train_dataset_after_pca.csv\")\n",
    "eval_df = pd.read_csv(\"../etl/val_dataset_after_pca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.2\n",
    "reward_multiplicator = 144.5\n",
    "negative_reward_multiplicator = 140.6\n",
    "\n",
    "train_py_env = CustomEnv(\n",
    "    trades = train,\n",
    "    features = [c for c in train.columns.values if \"f_\" in c] + [\"feature_0\", \"weight\"],\n",
    "    reward_column = \"resp\",\n",
    "    weight_column = \"weight\",\n",
    "    include_weight=True,\n",
    "    reward_multiplicator = reward_multiplicator,\n",
    "    negative_reward_multiplicator = negative_reward_multiplicator\n",
    ")\n",
    "\n",
    "eval_py_env = CustomEnv(\n",
    "    trades = train,\n",
    "    features = [c for c in train.columns.values if \"f_\" in c] + [\"feature_0\", \"weight\"],\n",
    "    reward_column = \"resp\",\n",
    "    weight_column = \"weight\",\n",
    "    include_weight=True,\n",
    "    reward_multiplicator = 1,\n",
    "    negative_reward_multiplicator = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_u_metric(df, model):\n",
    "    print(\"evaluating policy\")\n",
    "\n",
    "\n",
    "    actions = model.predict(df[[c for c in df.columns if \"f_\" in c] + [\"feature_0\",\"weight\"]].values)[0]\n",
    "    assert not np.isnan(np.sum(actions))\n",
    "\n",
    "    sum_of_actions = np.sum(actions)\n",
    "    print(\"np_sum(actions)\", sum_of_actions)\n",
    "\n",
    "#     df[\"action\"] = probs_df[\"action\"]\n",
    "    df[\"action\"] = pd.Series(data=actions, index=df.index)\n",
    "\n",
    "    df[\"trade_reward\"] = df[\"action\"]*df[\"weight\"]*df[\"resp\"]\n",
    "\n",
    "    tmp = df.groupby([\"date\"])[[\"trade_reward\"]].agg(\"sum\")\n",
    "\n",
    "    sum_of_pi = tmp[\"trade_reward\"].sum()\n",
    "    sum_of_pi_x_pi = (tmp[\"trade_reward\"]*tmp[\"trade_reward\"]).sum()\n",
    "\n",
    "    print(\"sum of pi: {sum_of_pi}\".format(sum_of_pi = sum_of_pi) )\n",
    "\n",
    "    t = sum_of_pi/np.sqrt(sum_of_pi_x_pi) * np.sqrt(250/tmp.shape[0])\n",
    "    print(\"t: {t}\".format(t = t) )\n",
    "\n",
    "    u  = np.min([np.max([t, 0]), 6]) * sum_of_pi\n",
    "    print(\"u: {u}\".format(u = u) )\n",
    "    ratio_of_ones = sum_of_actions/len(actions)\n",
    "    print(\"ration of ones\", ratio_of_ones)\n",
    "    print(\"length of df\", len(actions))\n",
    "\n",
    "    print(\"finished evaluating policy\")\n",
    "\n",
    "    return t, u, ratio_of_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = DummyVecEnv([lambda: train_py_env])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', train_env, verbose=1)\n",
    "model.learn(total_timesteps=2.4e6)\n",
    "\n",
    "\n",
    "print(calculate_u_metric(eval_df, model))\n",
    "print(calculate_u_metric(train, model))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[[c for c in train.columns.values if \"f_\" in c] + [\"feature_0\", \"weight\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(eval_df[[c for c in train.columns.values if \"f_\" in c] + [\"feature_0\", \"weight\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_u_metric(eval_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
